I just figured out how to run GPU programs at the CRC server: we need to enter a "gpu stage" first.

Batch job template:

#SBATCH --job-name=gpus-1
#SBATCH --output=gpus-1.out
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cluster=gpu
#SBATCH --partition=gtx1080
#SBATCH --gres=gpu:1
#SBATCH --time=24:00:00

Template command in terminal:

(base) [xid40@login1 ~]$ srun -M gpu -c 2 --partition=titanx --time=1:00:00 --gres=gpu:1 --pty bash



srun: job 241445 queued and waiting for resources

srun: job 241445 has been allocated resources




(base) [xid40@gpu-stage02 ~]$ ls








when I try to run the following code: (base) [xid40@gpu-stage06 PyTorch-VAE]$ python run.py -c configs/vae.yaml

I got the following error, it looks like I did not get GPU allocated, although I am in GPU mode

pytorch_lightning.utilities.exceptions.MisconfigurationException: You requested GPUs: [1]

But your machine only has: [0]

Comments
Submitted by FANGPING on Mon, 01/03/2022 - 18:09 Permalink

new
Assigned: unassigned -> xid40
Client: -> support
State: -> active
Priority: -> normal
I believe that the error message is self-explanatory. Your configuration file has specified gpus: [1]. When you requested a single GPU card, the gpu index is 0.

trainer_params:
gpus: [0]



(base) [xid40@login0 ~]$ crc-interactive.py --help
 crc-interactive.py -- An interactive Slurm helper
Usage:
    crc-interactive.py (-s | -g | -m | -i | -d) [-hvzo] [-t <time>] [-n <num-nodes>]
        [-p <partition>] [-c <num-cores>] [-u <num-gpus>] [-r <res-name>]
        [-b <memory>] [-a <account>] [-l <license>] [-f <feature>]

Positional Arguments:
    -s --smp                        Interactive job on smp cluster
    -g --gpu                        Interactive job on gpu cluster
    -m --mpi                        Interactive job on mpi cluster
    -i --invest                     Interactive job on invest cluster
    -d --htc                        Interactive job on htc cluster
Options:
    -h --help                       Print this screen and exit
    -v --version                    Print the version of crc-interactive.py
    -t --time <time>                Run time in hours, 1 <= time <= 12 [default: 1]
    -n --num-nodes <num-nodes>      Number of nodes [default: 1]
    -p --partition <partition>      Specify non-default partition
    -c --num-cores <num-cores>      Number of cores per node [default: 1]
    -u --num-gpus <num-gpus>        Used with -g only, number of GPUs [default: 0]
    -r --reservation <res-name>     Specify a reservation name
    -b --mem <memory>               Memory in GB
    -a --account <account>          Specify a non-default account
    -l --license <license>          Specify a license
    -f --feature <feature>          Specify a feature, e.g. `ti` for GPUs
    -z --print-command              Simply print the command to be run
    -o --openmp                     Run using OpenMP style submission

https://slurm.schedmd.com/gres.html




pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html
module load gcc/9.2.0

We cannot expand your home directory. Why cannot you use /ix/yufeihuang?

If you would like to access /ix/yufeihuang as a subfolder under your home directory, you can generate a soft link.

ln -s FILE LINK



FANGPING has updated the ticket 'docker installation':
https://crc.pitt.edu/node/10052#comment-26998
State:active
Priority:normal

Update text:
------------------------------

I don't find any problems to pull the docker image.

/ihome/crc/install/clairvoyance/clv.img

[fangping@login0b clairvoyance]$ module load singularity/3.8.3
[fangping@login0b clairvoyance]$ singularity pull --name clv.img docker://clairvoyancedocker/clv:latest
INFO: Converting OCI blobs to SIF format
INFO: Starting build...
Getting image source signatures
Copying blob 852e50cd189d skipped: already exists
Copying blob a6236801494d skipped: already exists
Copying blob 679c171d6942 skipped: already exists
Copying blob 4190bee14d68 done
Copying blob c11cf53cfc5c done
Copying blob dcd81d4244a6 done
Copying blob b22f769c41ee done
Copying blob 40f1339bbf34 done
Copying config 678966665f done
Writing manifest to image destination
Storing signatures
2022/01/24 10:12:42 info unpack layer: sha256:852e50cd189dfeb54d97680d9fa6bed21a6d7d18cfb56d6abfe2de9d7f173795
2022/01/24 10:12:43 info unpack layer: sha256:a6236801494d5ca9acfae6569427398c2942c031375b96cac887cafe1de5a09b
2022/01/24 10:12:46 info unpack layer: sha256:679c171d6942954a759f2d3a1dff911321940f23b0cdbe1d186f36bef0025124
2022/01/24 10:12:48 info unpack layer: sha256:4190bee14d68763bef0a7e007a8471f6bf5454c2e233cb9b1cee04fcf11bfb6b
2022/01/24 10:12:48 info unpack layer: sha256:c11cf53cfc5c93457c565ae46cb75ce000e77e2d22ff6dfdba44e2d85d7dda60
2022/01/24 10:12:48 info unpack layer: sha256:dcd81d4244a689d0556750c35796830880e33b1eb1839ee283abceffb31902c8
2022/01/24 10:12:48 info unpack layer: sha256:b22f769c41eed33ce881c20c5be3e8d69a9993b6d0a4c1163354aafed77852e8
2022/01/24 10:14:42 info unpack layer: sha256:40f1339bbf3493083f2fe339c4e8b1bb48e63ff75374de67063fbc720ff88a4a
INFO: Creating SIF file...
[fangping@login0b clairvoyance]$


FANGPING has updated the ticket 'space full again':
https://crc.pitt.edu/node/10053#comment-26996
State:active
Priority:normal

Update text:
------------------------------

You can install your own miniconda3 to /ihome/yufeihuang/xid40/xid40, which is the soft link to /ix folder. /ihome/yufeihuang/xid40/miniconda3 is ~55GB.

You can also generate conda envs using your /ix folder.

conda create --prefix=/ihome/yufeihuang/xid40/xid40/myenv python=3.7
source activate /ihome/yufeihuang/xid40/xid40/myenv

/ihome/yufeihuang/xid40/miniconda3/envs is ~36 GB.




