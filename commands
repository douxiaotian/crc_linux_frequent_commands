I just figured out how to run GPU programs at the CRC server: we need to enter a "gpu stage" first.

Batch job template:

#SBATCH --job-name=gpus-1
#SBATCH --output=gpus-1.out
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cluster=gpu
#SBATCH --partition=gtx1080
#SBATCH --gres=gpu:1
#SBATCH --time=24:00:00

Template command in terminal:

(base) [xid40@login1 ~]$ srun -M gpu -c 2 --partition=titanx --time=1:00:00 --gres=gpu:1 --pty bash



srun: job 241445 queued and waiting for resources

srun: job 241445 has been allocated resources




(base) [xid40@gpu-stage02 ~]$ ls








when I try to run the following code: (base) [xid40@gpu-stage06 PyTorch-VAE]$ python run.py -c configs/vae.yaml

I got the following error, it looks like I did not get GPU allocated, although I am in GPU mode

pytorch_lightning.utilities.exceptions.MisconfigurationException: You requested GPUs: [1]

But your machine only has: [0]

Comments
Submitted by FANGPING on Mon, 01/03/2022 - 18:09 Permalink

new
Assigned: unassigned -> xid40
Client: -> support
State: -> active
Priority: -> normal
I believe that the error message is self-explanatory. Your configuration file has specified gpus: [1]. When you requested a single GPU card, the gpu index is 0.

trainer_params:
gpus: [0]



(base) [xid40@login0 ~]$ crc-interactive.py --help
 crc-interactive.py -- An interactive Slurm helper
Usage:
    crc-interactive.py (-s | -g | -m | -i | -d) [-hvzo] [-t <time>] [-n <num-nodes>]
        [-p <partition>] [-c <num-cores>] [-u <num-gpus>] [-r <res-name>]
        [-b <memory>] [-a <account>] [-l <license>] [-f <feature>]

Positional Arguments:
    -s --smp                        Interactive job on smp cluster
    -g --gpu                        Interactive job on gpu cluster
    -m --mpi                        Interactive job on mpi cluster
    -i --invest                     Interactive job on invest cluster
    -d --htc                        Interactive job on htc cluster
Options:
    -h --help                       Print this screen and exit
    -v --version                    Print the version of crc-interactive.py
    -t --time <time>                Run time in hours, 1 <= time <= 12 [default: 1]
    -n --num-nodes <num-nodes>      Number of nodes [default: 1]
    -p --partition <partition>      Specify non-default partition
    -c --num-cores <num-cores>      Number of cores per node [default: 1]
    -u --num-gpus <num-gpus>        Used with -g only, number of GPUs [default: 0]
    -r --reservation <res-name>     Specify a reservation name
    -b --mem <memory>               Memory in GB
    -a --account <account>          Specify a non-default account
    -l --license <license>          Specify a license
    -f --feature <feature>          Specify a feature, e.g. `ti` for GPUs
    -z --print-command              Simply print the command to be run
    -o --openmp                     Run using OpenMP style submission

https://slurm.schedmd.com/gres.html
